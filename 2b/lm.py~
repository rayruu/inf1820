# encoding: utf-8

import nltk
from nltk import bigrams
from nltk.probability import ConditionalFreqDist, FreqDist, ConditionalProbDist, LaplaceProbDist
from math import log

class LM:
    def __init__(self):
        self.bigrams = ConditionalFreqDist()
        self.unigrams = FreqDist()
        sentences = nltk.corpus.brown.sents(categories=nltk.corpus.brown.categories()[1:])

        for sent in sentences:
            # Vi utvider setningen med None foran, for å angi start av
            # setningen, og en None etter, for å markere setningsslutt.
            sent = [None] + sent + [None]
            for prev, word in bigrams(sent):
                self.bigrams[prev][word] += 1
                self.unigrams[word] += 1
                
        self.bigrams = ConditionalProbDist(self.bigrams, LaplaceProbDist)
        self.unigrams = LaplaceProbDist(self.unigrams)

    def p(self, w, prev):
        p = 0.5*self.unigrams.prob(w)
        if prev in self.bigrams:
            p += self.bigrams[prev].prob(w)
        return p

    def logprob(self, s):
        P = 0.0
        for prevWord, word in bigrams(s):
            P += log(self.p(word, prevWord), 2)
            
        return P

    def perplexity(self, sents):
        N = 0.0
        l = 0.0
        
        for sent in sents:
            N += len(sent)
            l += self.logprob(sent)
        
        return 2**(-l/N)

    def zipfity(self, lst):
        frekvensOrd = FreqDist()
        teoretiskFrekv = 0.0
        N = 0.0
        
        frekvensOrd = FreqDist(word.lower() for word in lst)
                
        
        
        return frekvensOrd.most_common(10)
        
